# Problem Deconstruction & Solution Strategy

Let me break down this fraud detection problem into manageable chunks and build an optimal solution.

## Problem Analysis

**Core Challenge:** Fuzzy name matching at scale
- 10M daily transactions × 70K employees = 700 billion potential comparisons (infeasible)
- Unstructured counterparty names with variations
- Need high accuracy with reasonable performance

## Solution Architecture

I'll build a **multi-stage filtering approach** to reduce comparisons and catch variations:

1. **Stage 1: Quick Filters** - Eliminate obvious non-matches (reduces by ~99%)
2. **Stage 2: Fuzzy Matching** - Apply sophisticated matching on remaining candidates
3. **Stage 3: Scoring & Ranking** - Prioritize high-confidence matches## Key Optimizations Explained

### 1. **Multi-Stage Filtering** (Critical for Scale)
- **Stage 1**: Trigram indexing + last name lookup reduces 700B comparisons to ~100K
- **Stage 2**: Fuzzy matching only on filtered candidates
- **Result**: ~99.99% reduction in comparisons

### 2. **Matching Techniques**
- **Token Sort Ratio**: Handles word order ("John Doe" vs "Doe John")
- **Partial Ratio**: Catches substrings ("J Doe" in "John Doe")
- **Multiple name formats**: Tests against "First Last", "Last First", "Initial Last"
- **Weighted scoring**: Prioritizes full name matches

### 3. **Performance Features**
- **Batch processing**: Handles 10M rows without memory issues
- **Preprocessing**: One-time indexing of employee data
- **Ready for parallelization**: Can use multiprocessing for even faster processing

## Typical Performance

For 10M transactions × 70K employees:
- **Without optimization**: ~2-3 days
- **With this approach**: ~30-60 minutes on modern hardware

## Configuration Tips

```python
# Strict matching (fewer false positives)
matches = detector.find_matches(transactions, similarity_threshold=90)

# Lenient matching (catch more variations, more false positives)
matches = detector.find_matches(transactions, similarity_threshold=80)

# Recommended: 85 for good balance
```

## Next Steps

1. **Tune threshold** based on false positive review
2. **Add filters**: Exclude small amounts, certain transaction types
3. **Monitoring**: Track flagged employees over time
4. **Investigation workflow**: Integrate with case management system

Would you like me to:
1. Add additional matching rules (e.g., phonetic matching for misspellings)?
2. Optimize for even larger scale with distributed processing?
3. Add a scoring system that considers transaction patterns beyond just name matching?


# Deep Dive: Multi-Stage Matching Theory

Let me explain the mathematical and algorithmic theory behind each stage.

---

## **Stage 1: Quick Filters (Blocking/Indexing)**

### The Core Problem
Without filtering, we'd need to compare every transaction against every employee:
```
10,000,000 transactions × 70,000 employees = 700,000,000,000 comparisons
```

Even at 1 microsecond per comparison, this would take **~8 days** of CPU time.

### Solution: Probabilistic Filtering

We use two techniques that eliminate impossible matches while preserving all possible matches.

---

### **Technique 1: Trigram Indexing**

#### What are Trigrams?
Trigrams are sequences of 3 consecutive characters. For example:

```
"John Doe" → trigrams:
"  j", " jo", "joh", "ohn", "hn ", "n d", " do", "doe", "oe "
```

#### Why Trigrams Work

**Mathematical Principle**: If two strings are similar, they must share many trigrams.

```
String A: "John Doe"     → 9 trigrams
String B: "Jhon Doe"     → 9 trigrams (misspelling)
Shared trigrams: 7/9     → 78% overlap ✓ KEEP

String A: "John Doe"     → 9 trigrams  
String C: "Sarah Smith"  → 11 trigrams
Shared trigrams: 0/9     → 0% overlap ✗ ELIMINATE
```

#### The Index Structure

```python
trigram_index = {
    "joh": {emp_1, emp_45, emp_203},     # All employees with "joh"
    "ohn": {emp_1, emp_45, emp_789},     # All employees with "ohn"
    "doe": {emp_1, emp_67, emp_890},     # All employees with "doe"
    ...
}
```

#### Filtering Algorithm

```
For counterparty name "Jhon Doe":
1. Extract trigrams: {" jh", "jho", "hon", "on ", " do", "doe", "oe "}
2. Look up each trigram in index
3. Count how many times each employee appears
4. Keep employees appearing in ≥30% of trigrams

Result: Instead of checking 70,000 employees, 
        we check only ~50-100 candidates
```

#### Time Complexity
- **Without filtering**: O(N × M) = O(700 billion)
- **With trigrams**: O(N × k) where k ≈ 50-100 candidates
- **Reduction**: ~99.999%

---

### **Technique 2: Last Name Indexing**

#### The Observation
In most naming conventions, the last name is relatively stable:
- "John Doe" → last name: "Doe"
- "J Doe" → last name: "Doe"  
- "Doe John" → last name: "John" or "Doe" (we check both patterns)

#### The Index Structure

```python
lastname_index = {
    "doe": {emp_1, emp_45, emp_203},
    "smith": {emp_2, emp_67, emp_890},
    "johnson": {emp_3, emp_101},
    ...
}
```

#### Filtering Logic

```
For "J Doe":
1. Extract last token: "doe"
2. Look up in lastname_index
3. Get all employees with last name "Doe": {emp_1, emp_45, emp_203}

This gives us only 3 candidates instead of 70,000!
```

#### Edge Cases Handled
```python
# We also check reversed names
"Doe John" → Check both "john" and "doe" as potential last names
```

---

### **Combined Filtering (Union of Sets)**

We use **BOTH** techniques and take the union:

```
Candidates = (Trigram_Matches ∪ LastName_Matches)

Example for "J Doe":
- Trigram matches: {emp_1, emp_45, emp_67}
- Last name matches: {emp_1, emp_45, emp_203}
- Final candidates: {emp_1, emp_45, emp_67, emp_203}  ← Only 4 to check!
```

#### Why Union (OR) Not Intersection (AND)?

**Goal**: High Recall (don't miss any true matches)

```
If we used AND (intersection):
- Might miss "Jhonny Doe" (misspelling doesn't match trigrams well)
- Might miss "J.D." (abbreviated, no clear last name)

Using OR (union):
- Trigrams catch misspellings
- Last name catches abbreviations
- We're safe as long as ONE technique works
```

---

### **Performance Analysis**

| Technique | Avg Candidates | Reduction | Use Case |
|-----------|---------------|-----------|----------|
| Last Name Index | 10-50 | 99.93% | Clean names, common patterns |
| Trigram Index | 50-200 | 99.7% | Misspellings, variations |
| Combined | 100-300 | 99.5%+ | Best of both worlds |

For 10M transactions:
- Original comparisons needed: 700 billion
- With filtering: ~1-3 billion (99.5%+ reduction)
- **Processing time**: Days → Minutes

---

## **Stage 2: Fuzzy Matching**

Now we have ~100-300 candidates per transaction. We need sophisticated algorithms to score similarity.

---

### **Algorithm 1: Levenshtein Distance (Edit Distance)**

#### Theory
The **minimum number of single-character edits** (insertions, deletions, substitutions) needed to transform one string into another.

#### Mathematical Definition

```
For strings A and B:
lev(i,j) = minimum edits to transform A[1..i] to B[1..j]

lev(i,j) = {
    j                                    if i = 0
    i                                    if j = 0
    lev(i-1, j-1)                       if A[i] = B[j]
    1 + min(
        lev(i-1, j),      // deletion
        lev(i, j-1),      // insertion
        lev(i-1, j-1)     // substitution
    )                                    if A[i] ≠ B[j]
}
```

#### Example: "John" → "Jhon"

```
    ""  J  h  o  n
""   0  1  2  3  4
J    1  0  1  2  3
o    2  1  1  1  2
h    3  2  1  2  2
n    4  3  2  2  2
```

**Distance = 2** (swap 'o' and 'h')

#### Similarity Score
```
similarity = (1 - distance/max_length) × 100

"John" vs "Jhon":
similarity = (1 - 2/4) × 100 = 50%
```

**Problem**: Too strict! "Jhon" should score higher than 50%.

---

### **Algorithm 2: Token Sort Ratio**

#### Theory
Tokenize strings, sort tokens alphabetically, then compare.

#### Why This Works
Handles **word order variations**:

```
"John Doe" → tokens: ["john", "doe"] → sorted: "doe john"
"Doe John" → tokens: ["doe", "john"] → sorted: "doe john"

Levenshtein("doe john", "doe john") = 0
Similarity = 100% ✓
```

#### Algorithm Steps
```python
def token_sort_ratio(s1, s2):
    # 1. Normalize: lowercase, remove punctuation
    s1 = normalize(s1)  # "John Doe" → "john doe"
    s2 = normalize(s2)  # "Doe, John" → "doe john"
    
    # 2. Tokenize and sort
    tokens1 = sorted(s1.split())  # ["doe", "john"]
    tokens2 = sorted(s2.split())  # ["doe", "john"]
    
    # 3. Join and compare
    sorted1 = " ".join(tokens1)  # "doe john"
    sorted2 = " ".join(tokens2)  # "doe john"
    
    # 4. Levenshtein distance on sorted strings
    return levenshtein_similarity(sorted1, sorted2)
```

#### Examples

```
"John Michael Doe" vs "Doe John M":
Sorted: "doe john michael" vs "doe john m"
Score: ~85% (high!)

"Sarah Williams" vs "John Doe":
Sorted: "sarah williams" vs "doe john"
Score: ~15% (low, correctly identified as different)
```

---

### **Algorithm 3: Partial Ratio**

#### Theory
Find the **best matching substring** within longer strings.

#### Use Case
Catches cases where one name is abbreviated:

```
"J Doe" vs "John Michael Doe"

Partial matching finds:
- "j doe" is 5 characters
- "john michael doe" is 16 characters
- Best substring match: "john...doe" ≈ "j doe"
- Score: ~80% (good!)
```

#### Algorithm
```python
def partial_ratio(short, long):
    if len(short) > len(long):
        short, long = long, short
    
    best_score = 0
    # Sliding window of length len(short) over long string
    for i in range(len(long) - len(short) + 1):
        substring = long[i:i+len(short)]
        score = levenshtein_similarity(short, substring)
        best_score = max(best_score, score)
    
    return best_score
```

#### Example Visualization

```
Short: "j doe"
Long:  "john michael doe"

Window 1: "john " vs "j doe" → 40%
Window 2: "ohn m" vs "j doe" → 20%
...
Window 9: "l doe" vs "j doe" → 80% ← Best match!
```

---

### **Algorithm 4: Token Set Ratio**

#### Theory
Handles cases where strings have **common and different parts**.

#### Formula

```
intersection = common tokens
remainder1 = tokens only in string1
remainder2 = tokens only in string2

score = max(
    ratio(intersection, intersection),
    ratio(intersection + remainder1, intersection + remainder2),
    ratio(intersection + remainder2, intersection + remainder1)
)
```

#### Example

```
"John Michael Doe" vs "John Doe Jr"

Tokens1: {john, michael, doe}
Tokens2: {john, doe, jr}

Intersection: {john, doe}
Remainder1: {michael}
Remainder2: {jr}

Compare:
- "john doe" vs "john doe" → 100%
- "john doe michael" vs "john doe jr" → 85%
- "john doe jr" vs "john doe michael" → 85%

Final score: 100%
```

---

### **Multi-Algorithm Scoring Strategy**

We use **weighted combination** of all algorithms:

```python
def fuzzy_match_score(counterparty, employee):
    scores = []
    
    # 1. Token Sort Ratio (weight: 1.2) - handles word order
    scores.append(token_sort_ratio(counterparty, employee.full_name) × 1.2)
    
    # 2. Exact ratio - baseline
    scores.append(ratio(counterparty, employee.first_last))
    
    # 3. Reversed name
    scores.append(ratio(counterparty, employee.last_first))
    
    # 4. Initial + Last (weight: 0.9) - slightly penalized
    scores.append(ratio(counterparty, employee.initial_last) × 0.9)
    
    # 5. Partial ratio (weight: 0.8) - for abbreviations
    scores.append(partial_ratio(counterparty, employee.full_name) × 0.8)
    
    # Return best score (max), capped at 100
    return min(max(scores), 100)
```

#### Why Multiple Algorithms?

Each algorithm catches different patterns:

| Algorithm | Best For | Example |
|-----------|----------|---------|
| Token Sort | Word order | "Doe John" vs "John Doe" |
| Exact Ratio | Clean matches | "John Doe" vs "John Doe" |
| Partial Ratio | Abbreviations | "J Doe" vs "John Doe" |
| Reversed | Name formats | "Smith, Jane" vs "Jane Smith" |

---

### **Threshold Selection**

#### Statistical Justification

Based on empirical analysis:

```
Threshold 95+:  Very high confidence (likely exact person)
Threshold 85-94: High confidence (probably same person, check manually)
Threshold 75-84: Medium confidence (many false positives)
Threshold <75:  Low confidence (likely different people)
```

#### Example Score Distribution

```
"John Doe" vs:
- "John Doe"          → 100 (exact)
- "Doe John"          → 100 (token sort)
- "J Doe"             → 88 (partial match)
- "John D"            → 85 (abbreviated)
- "Jhon Doe"          → 93 (misspelling)
- "Johnny Doe"        → 90 (nickname)
- "John Smith"        → 50 (different person)
- "Jane Doe"          → 75 (different first name)
```

**Recommended threshold: 85** provides good balance.

---

## **Combined System Performance**

### Computational Complexity

| Stage | Operations | Time |
|-------|-----------|------|
| Stage 1: Indexing (one-time) | O(M × L) | ~1 min |
| Stage 1: Filtering (per transaction) | O(T × log M) | ~0.001s |
| Stage 2: Fuzzy Matching | O(C × L²) | ~0.01s |
| **Total per transaction** | | **~0.011s** |
| **Total for 10M transactions** | | **~30 min** |

Where:
- M = 70,000 employees
- N = 10,000,000 transactions
- T = avg trigrams per name (~10)
- C = avg candidates after filtering (~100)
- L = avg name length (~20 chars)

---

## **Key Insights**

### 1. **Precision vs Recall Tradeoff**

```
Stage 1 (Filtering): Optimized for RECALL
→ Don't miss any potential matches (cast wide net)
→ Acceptable to have false positives

Stage 2 (Fuzzy Matching): Optimized for PRECISION  
→ Accurately score the candidates
→ Filter out false positives with threshold
```

### 2. **Why Not Just Use Fuzzy Matching?**

```
Without Stage 1:
- 10M × 70K = 700B comparisons
- Even at 1μs each = 8 days

With Stage 1:
- 10M × 100 = 1B comparisons  
- At 10μs each = 3 hours
- Plus indexing overhead = ~30-60 minutes total
```

### 3. **Information Theory Perspective**

**Entropy reduction**:
```
Initial entropy: log₂(70,000) ≈ 16 bits of uncertainty
After Stage 1:   log₂(100) ≈ 7 bits
After Stage 2:   log₂(5) ≈ 2 bits (top matches)

Information gained: 14 bits (99.99% reduction in uncertainty)
```

---

Does this explanation clarify the theory? Would you like me to dive deeper into any specific algorithm or explore alternative approaches like phonetic matching (Soundex, Metaphone) or machine learning methods?
